---
title: "Week 7 - Homework"
author: "Saurav Prem Kaushik Chetry(schetry2@illinois.edu)"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (EPA Emissions Data)

For this exercise, we will use the data stored in [`epa2015.csv`](epa2015.csv). It contains detailed descriptions of 4,411 vehicles manufactured in 2015 that were used for fuel economy testing [as performed by the Environment Protection Agency]( https://www3.epa.gov/otaq/tcldata.htm). The variables in the dataset are:  

- `Make` - Manufacturer
- `Model` - Model of vehicle
- `ID` - Manufacturer defined vehicle identification number within EPA's computer system (not a VIN number)
- `disp` - Cubic inch displacement of test vehicle
- `type` - Car, truck, or both (for vehicles that meet specifications of both car and truck, like smaller SUVs or crossovers)
- `horse` - Rated horsepower, in foot-pounds per second
- `cyl` - Number of cylinders
- `lockup` - Vehicle has transmission lockup; N or Y
- `drive` - Drivetrain system code
    - A = All-wheel drive
    - F = Front-wheel drive
    - P = Part-time 4-wheel drive
    - R = Rear-wheel drive
    - 4 = 4-wheel drive
- `weight` - Test weight, in pounds
- `axleratio` - Axle ratio
- `nvratio` - n/v ratio (engine speed versus vehicle speed at 50 mph)
- `THC` - Total hydrocarbons, in grams per mile (g/mi)
- `CO` - Carbon monoxide (a regulated pollutant), in g/mi
- `CO2` - Carbon dioxide (the primary byproduct of all fossil fuel combustion), in g/mi
- `mpg` - Fuel economy, in miles per gallon

We will attempt to model `CO2` using both `horse` and `type`. In practice, we would use many more predictors, but limiting ourselves to these two, one numeric and one factor, will allow us to create a number of plots.

Load the data, and check its structure using `str()`. Verify that `type` is a factor; if not, coerce it to be a factor.

```{r}
w07_Q1 = read.csv("epa2015.csv")
str(w07_Q1)
#is.factor(w07_Q1$type)
```
Is 'type' a factor? : <tt>`r is.factor(w07_Q1$type)`</tt>

**(a)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit a simple linear regression model with `CO2` as the response and only `horse` as the predictor.
- Add the fitted regression line to the scatterplot. Comment on how well this line models the data.
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. (Interestingly, the dataset gives the wrong drivetrain for most Subarus in this dataset, as they are almost all listed as `F`, when they are in fact all-wheel drive.)

SOLUTION: (a)

```{r message=FALSE, warning=FALSE}
w07_Q1_m1 = lm(CO2 ~ horse, data = w07_Q1)
plot_colors = c("Red", "Darkgrey", "Dodgerblue")
plot(CO2~horse, data = w07_Q1, col = plot_colors[type], pch = as.numeric(type), main = "CO2 Vs Horsepower", xlab="horsepower in foot-pounds per second", cex = 0.5)
legend("topleft", c("Both", "Car", "Truck"),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))
abline(w07_Q1_m1, col = "black", lty = 1, lwd = 2)
```


It is observed that red"Both",blue "Truck" & grey "Car" types fall above the line. This means the simple linear model  underestimates the Car, Truck and Both types.

```{r}
summary(w07_Q1_m1)$coefficients[2,1]
```

As different vehicle types, would still have the same slope $\beta_1$ with this simple linear regression model.
Estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`is 
<tt>`r summary(w07_Q1_m1)$coefficients[2,1]`</tt>

$$
\hat\beta_1 = `r summary(w07_Q1_m1)$coefficients[2,1]`
$$


90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both` is:


```{r}
new_data = data.frame(horse = c(148))
predict(w07_Q1_m1, newdata = new_data, interval = "prediction", level = 0.90)
```


Since, the SLR modeldoes not take into consideration the make and model of the vehicle, this PI would be true for any vehicle with a horsepower of 148


**(b)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit an additive multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 


SOLUTION: (b)

R has chosen the following model:$Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon$
$v_1$ =  Both, $v_2$ = Car and $v3$ = Truck are dummy variables, and $v_1$ is the reference. $v_2$ and $v_3$ would be used to create three lines.

So, the 3 submodels for each type of vehicle are:

- Both: $Y = \beta_0 + \beta_1 x + \epsilon$
- Car: $Y = (\beta_0 + \beta_2) + \beta_1 x + \epsilon$
- Truck: $Y = (\beta_0 + \beta_3) + \beta_1 x + \epsilon$

Based on the submodels let us plot the MLR model

```{r}
w07_Q1_m2 = lm(CO2 ~ horse + type, data = w07_Q1)
#summary(w07_Q1_m2)$coefficients

int_both = coef(w07_Q1_m2)[1]
int_car = coef(w07_Q1_m2)[1] + coef(w07_Q1_m2)[3]
int_truck = coef(w07_Q1_m2)[1] + coef(w07_Q1_m2)[4]

slope_all_types = coef(w07_Q1_m2)[2]

plot_colors = c("Red", "Darkgrey", "Dodgerblue")
plot(CO2~horse, data = w07_Q1, col = plot_colors[type], pch = as.numeric(type), main = "CO2 Vs Horse", xlab="horsepower in foot-pounds per second", cex = 0.5)

abline(int_both, slope_all_types, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_car, slope_all_types, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_truck, slope_all_types, col = plot_colors[3], lty = 3, lwd = 2)
legend("topleft", c("Both", "Car", "Truck"),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))

# Finding out if the plotted lines from the submodel do justice to the data.

#str(w07_data)
Both = subset(w07_Q1, w07_Q1$type == "Both")
Car = subset(w07_Q1, w07_Q1$type == "Car")
Truck = subset(w07_Q1, w07_Q1$type == "Truck")

#(CO2 = c("Both" = mean(Both$CO2), "Car" = mean(Car$CO2), "Truck" = mean(Truck$CO2)))

```


Observation1: The lines look Ok except for the problem that, the average increase in CO2 for a unit change in horsepower, is same for all types of vehicles. This is not practical because vehicles of different types would have different CO2 values at the same horsepower.Because there are other factors affecting the CO2, viz. number of cylinders/engine type and external conditions. More details about the factors affecting CO2 are described in this study: http://www.supercomputingchallenge.org/14-15/finalreports/90.pdf

Observation2: The mean of CO2 emissions for types Both, Car, Truck respectively is:
$$
`r c("Both" = mean(Both$CO2), "Car" = mean(Car$CO2), "Truck" = mean(Truck$CO2))`
$$

At a given horsepower, trucks have highest CO2, but looking at the mean of the CO2 for "Both" and "Car", the lines are not correct. Here we are estimating that the type "Both" have higher CO2 emissions compared to CO2 emissions of type "Car". Whereas actually, the line for type "Car" should have been above the line of type "Both"

From the fitted model above, the estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`is:<tt>`r coef(w07_Q1_m2)[2]`</tt>

90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both` is:

```{r}
new_data = data.frame(horse = c(148), type = c("Both"))
predict(w07_Q1_m2, newdata = new_data, interval = "prediction", level = 0.90)
```

Since, the additive MLR model does not take into consideration the make and model of the vehicle, this PI would be true for any vehicle with a horsepower of 148 and type "Both"

**(c)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`. 
- Fit an interaction multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 

SOLUTION:(c)

R has chosen the following model:$Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon$
I am using $??$ like a $??$ parameter for simplicity, so that, for example $??_2$ and $??_2$ are both associated with $v_2$.

$v_1$ =  Both, $v_2$ = Car and $v_3$ = Truck are dummy variables, and $v_1$ is the reference. $v_2$ and $v_3$ would be used to create three lines.

So, the 3 submodels for each type of vehicle are:

- Both: $Y = \beta_0 + \beta_1 x + \epsilon$
- Car: $Y = (\beta_0 + \beta_2) + (\beta_1 + \gamma_2) x + \epsilon$
- Truck: $Y = (\beta_0 + \beta_3) + (\beta_1 + \gamma_3) x + \epsilon$

Based on the submodels let us plot the MLR model

```{r}
w07_Q1_m3 = lm(CO2 ~ horse * type, data = w07_Q1)
#summary(w07_Q1_m2)$coefficients

int_both = coef(w07_Q1_m3)[1]
int_car = coef(w07_Q1_m3)[1] + coef(w07_Q1_m3)[3]
int_truck = coef(w07_Q1_m3)[1] + coef(w07_Q1_m3)[4]

slope_both = coef(w07_Q1_m3)[2]
slope_car  = coef(w07_Q1_m3)[2] + coef(w07_Q1_m3)[5]
slope_truck= coef(w07_Q1_m3)[2] + coef(w07_Q1_m3)[6]

plot_colors = c("Red", "Darkgrey", "Dodgerblue")
plot(CO2~horse, data = w07_Q1, col = plot_colors[type], pch = as.numeric(type),main = "CO2 Vs Horse", xlab="horsepower in foot-pounds per second", cex = 0.5)

abline(int_both, slope_both, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_car, slope_car, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_truck, slope_truck, col = plot_colors[3], lty = 3, lwd = 2)
legend("topleft", c("Both", "Car", "Truck"),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))

# Finding out if the plotted lines from the submodel do justice to the data.

#str(w07_Q1)
Both = subset(w07_Q1, w07_Q1$type == "Both")
Car = subset(w07_Q1, w07_Q1$type == "Car")
Truck = subset(w07_Q1, w07_Q1$type == "Truck")

#(CO2 = c("Both" = mean(Both$CO2), "Car" = mean(Car$CO2), "Truck" = mean(Truck$CO2)))
```
Observation1: With the Intercation MLR model, we have different slopes for different vehicle types and their CO2 emissions at a certain horesepower.The new lines do address the problem observerd with additive MLR model previously. Now this model correctly displays the rate of change of CO2 for different vehicle types is different at the same horsepower.

Observation2: However, the interaction MLR does not yet solve the problem associated with additive MLR Observation2 above, i.e. position of the line for type "Car" is still below the line for type "Both". This could be a correct plot for the line and the premise that the positions should be different could be wrong.

From the fitted model above, the estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`is:<tt>`r coef(w07_Q1_m3)[2] + coef(w07_Q1_m3)[5]`</tt>

90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both` is:

```{r}
new_data = data.frame(horse = c(148), type = c("Both"))
predict(w07_Q1_m3, newdata = new_data, interval = "prediction", level = 0.90)
```

Since, the interaction MLR model does not take into consideration the make and model of the vehicle, this PI would be true for any vehicle with a horsepower of 148 and type "Both"


**(d)** Based on the previous plots, you probably already have an opinion on the best model. Now use an ANOVA $F$-test to compare the additive and interaction models. Based on this test and a significance level of $\alpha = 0.10$, which model is preferred?

SOLUTION:(d)

For ANOVA $F$-test:

Null model = Additive MLR model = w07_Q1_m2
Full model = Interaction MLR model = w07_Q1_m3

```{r}
c('Pr(>F)',anova(w07_Q1_m2, w07_Q1_m3)[2,6])
```

$Pr(>F)$ value from the above ANOVA test is: <tt>`r anova(w07_Q1_m2, w07_Q1_m3)[2,6]`</tt>. As it is lesser than the significance level of $\alpha = 0.10$, the preferred model is INTERACTION MLR MODEL

***

## Exercise 2 (Hospital SUPPORT Data, White Blood Cells)

For this exercise, we will use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Age`, `Education`, `Income`, and `Sex` in an attempt to model `Blood`. Essentially, we are attempting to model white blood cell count using only demographic information.

**(a)** Load the data, and check its structure using `str()`. Verify that `Education`, `Income`, and `Sex` are factors; if not, coerce them to be factors. What are the levels of `Education`, `Income`, and `Sex`?

```{r}
w07_Q2 = read.csv("hospital.csv")
str(w07_Q2)
is.factor(w07_Q2$Education)
is.factor(w07_Q2$Income)
is.factor(w07_Q2$Sex)
levels(w07_Q2$Education)
levels(w07_Q2$Income)
levels(w07_Q2$Sex)
```
Is 'Education' a factor? : <tt>`r is.factor(w07_Q2$Education)`</tt>
Is 'Income' a factor? : <tt>`r is.factor(w07_Q2$Income)`</tt>
Is 'Sex' a factor? : <tt>`r is.factor(w07_Q2$Sex)`</tt>

Levels of 'Education' are: <tt>`r levels(w07_Q2$Education)`</tt>
Levels of 'Income' are: <tt>`r levels(w07_Q2$Income)`</tt>
Levels of 'Sex' are: <tt>`r levels(w07_Q2$Sex)`</tt>

**(b)** Fit an additive multiple regression model with `Blood` as the response using `Age`, `Education`, `Income`, and `Sex` as predictors. What does `R` choose as the reference level for `Education`, `Income`, and `Sex`?

```{r}
w07_Q2_m1 = lm(Blood ~ Age + Education + Income + Sex, data =  w07_Q2)
summary(w07_Q2_m1)$coefficients
```
R chooses:
reference level "high" for Education
reference level "high" for Income
reference level "female" for Sex 


**(c)** Fit a multiple regression model with `Blood` as the response. Use the main effects of `Age`, `Education`, `Income`, and `Sex`, as well as the interaction of `Sex` with `Age` and the interaction of `Sex` and `Income`. Use a statistical test to compare this model to the additive model using a significance level of $\alpha = 0.10$. Which do you prefer?


```{r}
w07_Q2_m2 = lm(Blood ~ Age + Education + Income + Sex + Sex:Age + Sex:Income, data = w07_Q2)
summary(w07_Q2_m2)
anova(w07_Q2_m1, w07_Q2_m2)[2,6]
```
The $Pr(>F)$ value of the anova test between the additive model and this model is: <tt>`r anova(w07_Q2_m1, w07_Q2_m2)[2,6]`</tt>
As the $Pr(>F)$ value of this test is greater than the significance level $\alpha = 0.10$, the model I prefer is this model of (c) w07_Q2_m2

**(d)** Fit a model similar to that in **(c)**, but additionally add the interaction between `Income` and `Age` as well as a three-way interaction between `Age`, `Income`, and `Sex`. Use a statistical test to compare this model to the preferred model from **(c)** using a significance level of $\alpha = 0.10$. Which do you prefer?

```{r}
w07_Q2_m3 = lm(Blood ~ Age + Education + Income + Sex + Sex:Age + Sex:Income + Income:Age + Age:Income:Sex, data = w07_Q2)
#w07_Q2_m3 = lm(Blood ~ Education + Age*Income*Sex, data = w07_Q2)
#summary(w07_Q2_m3)
anova(w07_Q2_m2, w07_Q2_m3)[2,6]
```
The $Pr(>F)$ value of the anova test between the additive model and this model is: <tt>`r anova(w07_Q2_m2, w07_Q2_m3)[2,6]`</tt>
As the $Pr(>F)$ value of this test is greater than the significance level $\alpha = 0.10$, the model I prefer is this model of (d) w07_Q2_m3

**(e)** Using the model in **(d)**, give an estimate of the change in average `Blood` for a one-unit increase in `Age` for a highly educated, low income, male patient.

```{r}
summary(w07_Q2_m3)$coefficients
#coef(w07_Q2_m3)[3]
```
Model chosen by R for this interaction is : 
$Y = \beta_0  + \beta_1 x + \beta_2 u_1 + \beta_3 v_1 + \beta_4 w_1 + \beta_5 x w_1 + \beta_6 v_1 w_1 + \beta_7 x v_1  + \beta_8 x v_1 w_1  + \epsilon$

$x$ = age variable
$u_1$ = low education $u_2$ = high education
$v_1$ = low income $v_2$ = high income
$w_1$ = male $w_2$ = female

Slope of Age is given by: $x(\beta_1  + \beta_5 w_1 + \beta_7 v_1  + \beta_8 v_1 w_1)$
Replacing the variable values for Highly educated, low income, male patient, slope  = $x(\beta_1  + \beta_5  + \beta_7 + \beta_8)$

Therefore, estimate of the change in average `Blood` for a one-unit increase in `Age` for a highly educated, low income, male patient is:
<tt>`r coef(w07_Q2_m3)[[2]] + coef(w07_Q2_m3)[[6]] + coef(w07_Q2_m3)[[8]] + coef(w07_Q2_m3)[[9]]`</tt>


```{r}
coef(w07_Q2_m3)[2] + coef(w07_Q2_m3)[6] + coef(w07_Q2_m3)[8] + coef(w07_Q2_m3)[9]
```

***

## Exercise 3 (Hospital SUPPORT Data, Stay Duration)

For this exercise, we will again use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Blood`, `Pressure`, and `Rate` in an attempt to model `Days`. Essentially, we are attempting to model the time spent in the hospital using only health metrics measured at the hospital.

Consider the model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon,
\]

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

**(a)** Fit the model above. Also fit a smaller model using the provided `R` code.

Use a statistical test to compare the two models. Report the following:

- The null and alternative hypotheses in terms of the model given in the exercise description
- The value of the test statistic
- The p-value of the test
- A statistical decision using a significance level of $\alpha = 0.10$
- Which model you prefer

SOLUTION: (a)
```{r}
w07_Q2_m4 = lm(Days ~ Blood*Pressure*Rate, data = w07_Q2)
#summary(w07_Q2_m4)

days_add = lm(Days ~ Pressure + Blood + Rate, data = w07_Q2)
anova(days_add, w07_Q2_m4)
anova(days_add, w07_Q2_m4)[2,5]
anova(days_add, w07_Q2_m4)[2,6]
```
Null Hypothesis: 

3 way & 2 way interactions in the given model are not required, i.e. $H_0: \beta_4\ = \beta_5\ = \beta_6\ = \beta_7\ = 0$.

Alternative Hypothesis:

3 way & 2 way interactions in the given model are required, i.e., $H_1: \beta_4\ \neq \beta_5\ \neq \beta_6\ \neq \beta_7\ \neq 0$.

Full model: $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon$.

Null model: $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$.

The value of test statistic is: <tt>`r anova(days_add, w07_Q2_m4)[2,5]`</tt>

The p-value of test statistic is: <tt>`r anova(days_add, w07_Q2_m4)[2,6]`</tt>

At significance level of $\alpha = 0.10$, null hypothesis is rejected because the p-value:<tt>`r anova(days_add, w07_Q2_m4)[2,6]`</tt> is lesser than the significance level.

I prefer the full(alternative) model, i.e. the model with 2-way and 3-way interactions as given in the description.

**(b)** Give an expression based on the model in the exercise description for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.

SOLUTION: (b)

Given model in Question is:

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon,
\]

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

Expression of the model for true change is:

\[
Y = \beta_0 + 10\beta_1  + 139\beta_2 + 1390\beta_4 + (\beta_3  + 10\beta_5 + 139\beta_6 + 1390\beta_7)x_3 + \epsilon
\]

where, the intercept is:$\beta_0 + 10\beta_1  + 139\beta_2 + 1390\beta_4$
and the slope is       :$\beta_3  + 10\beta_5 + 139\beta_6 + 1390\beta_7$ which signifies the true change in days per unit change in Rate.


**(c)** Give an expression based on the additive model in part **(a)** for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.

SOLUTION: (c)

Given model in Question is the additive model.

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon
\]

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

Expression of the model for true change is:


\[
Y = \beta_0 + 10\beta_1  + 139\beta_2 + \beta_3 x_3 + \epsilon
\]

where, the intercept is:$\beta_0 + 10\beta_1  + 139\beta_2$.
and the slope is       :$\beta_3$. which signifies the true change in days per unit change in Rate.

***

## Exercise 4 ($t$-test Is a Linear Model)

In this exercise, we will try to convince ourselves that a two-sample $t$-test assuming equal variance is the same as a $t$-test for the coefficient in front of a single two-level factor variable (dummy variable) in a linear model.

First, we set up the data frame that we will use throughout.

```{r}
n = 30

sim_data = data.frame(
  groups = c(rep("A", n / 2), rep("B", n / 2)),
  values = rep(0, n))
str(sim_data)
```

We will use a total sample size of `30`, `15` for each group. The `groups` variable splits the data into two groups, `A` and `B`, which will be the grouping variable for the $t$-test and a factor variable in a regression. The `values` variable will store simulated data.

We will repeat the following process a number of times.

```{r}
set.seed(420)
sim_data$values = rnorm(n, mean = 42, sd = 3.5) # simulate response data
summary(lm(values ~ groups, data = sim_data))
t.test(values ~ groups, data = sim_data, var.equal = TRUE)
```

We use `lm()` to test

\[
H_0: \beta_1 = 0
\]

for the model

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon
\]

where $Y$ is the values of interest, and $x_1$ is a dummy variable that splits the data in two. We will let `R` take care of the dummy variable.

We use `t.test()` to test

\[
H_0: \mu_A = \mu_B
\]

where $\mu_A$ is the mean for the `A` group, and $\mu_B$ is the mean for the `B` group.

The following code sets up some variables for storage.

```{r}
num_sims = 300
lm_t = rep(0, num_sims)
lm_p = rep(0, num_sims)
tt_t = rep(0, num_sims)
tt_p = rep(0, num_sims)
```

- `lm_t` will store the test statistic for the test $H_0: \beta_1 = 0$.
- `lm_p` will store the p-value for the test $H_0: \beta_1 = 0$.
- `tt_t` will store the test statistic for the test $H_0: \mu_A = \mu_B$.
- `tt_p` will store the p-value for the test $H_0: \mu_A = \mu_B$.

The variable `num_sims` controls how many times we will repeat this process, which we have chosen to be `300`.

**(a)** Set a seed equal to your birthday. Then write code that repeats the above process `300` times. Each time, store the appropriate values in `lm_t`, `lm_p`, `tt_t`, and `tt_p`. Specifically, each time you should use `sim_data$values = rnorm(n, mean = 42, sd = 3.5)` to update the data. The grouping will always stay the same.

SOLUTION: (a)


```{r}
birthday = 19850208
set.seed(birthday)

for (i in 1:num_sims){

sim_data$values = rnorm(n, mean = 42, sd = 3.5)
lm_func = summary(lm(values ~ groups, data = sim_data))
t_test = t.test(values ~ groups, data = sim_data, var.equal = TRUE)

lm_t[i] = lm_func$coefficients[2,3]
lm_p[i] = lm_func$coefficients[2,4]

tt_t[i] = t_test$statistic
tt_p[i] = t_test$p.value
}

```

**(b)** Report the value obtained by running `mean(lm_t == tt_t)`, which tells us what proportion of the test statistics is equal. The result may be extremely surprising!
```{r}
mean(lm_t == tt_t)
```
Value is: <tt>`r mean(lm_t == tt_t)`</tt>


**(c)** Report the value obtained by running `mean(lm_p == tt_p)`, which tells us what proportion of the p-values is equal. The result may be extremely surprising!

```{r}
mean(lm_p == tt_p)
```
Value is: <tt>`r mean(lm_p == tt_p)`</tt>

**(d)** If you have done everything correctly so far, your answers to the last two parts won't indicate the equivalence we want to show! What the heck is going on here? The first issue is one of using a computer to do calculations. When a computer checks for equality, it demands **equality**; nothing can be different. However, when a computer performs calculations, it can only do so with a certain level of precision. So, if we calculate two quantities we know to be analytically equal, they can differ numerically. Instead of `mean(lm_p == tt_p)` run `all.equal(lm_p, tt_p)`. This will perform a similar calculation, but with a very small error tolerance for each equality. What is the result of running this code? What does it mean?
```{r}
#mean(lm_p == tt_p)
all.equal(lm_p, tt_p)

```
Result of running all.equal is :<tt>`r all.equal(lm_p, tt_p)`</tt>
It means, although the proportion of the p-values is almost zero, but while we consider a very small error tolerance, the p-values from lm fitted model/Group A and 2 sample t-test/Group B are all equal.

**(e)** Your answer in **(d)** should now make much more sense. Then what is going on with the test statistics? Look at the values stored in `lm_t` and `tt_t`. What do you notice? Is there a relationship between the two? Can you explain why this is happening?
```{r}
mean(lm_t == tt_t)
all.equal(lm_t, tt_t)
#lm_t
#tt_t
#levels(sim_data$groups)
```
Comparing the t-values from lm simulated model/Group A and the 2 sample t-test/Group B, the magnitudes are same but the signs are different.
For the lm model, R chose the "A" group as the reference level:$Y = \beta_0 + \beta_1 x_1 + \epsilon$, where $x_1$ is the dummy variable. As, $x_1$ splits the data into two groups, it takes values 0 for A group and 1 for B group. The 2 sample t-test does not allow to use variables like regression does. This probably is the reason for opposite sign for the same magnitude seen for the values of lm_t and tt_t


